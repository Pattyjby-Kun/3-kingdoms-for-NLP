# src/vajirayana_scraper.py
import requests
from bs4 import BeautifulSoup
import os
import time

OUTPUT_DIR = r"..\data\chapters"
BASE_URL = "https://vajirayana.org/‡∏™‡∏≤‡∏°‡∏Å‡πä‡∏Å/‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà-{}"

def to_thai_num(num: int) -> str:
    """‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏•‡∏Ç‡∏≠‡∏≤‡∏£‡∏ö‡∏¥‡∏Å (1,2,3) ‚Üí ‡πÄ‡∏•‡∏Ç‡πÑ‡∏ó‡∏¢ (‡πë,‡πí,‡πì)"""
    arabic = "0123456789"
    thai = "‡πê‡πë‡πí‡πì‡πî‡πï‡πñ‡πó‡πò‡πô"
    return "".join(thai[arabic.index(ch)] for ch in str(num))


def scrape_chapter(chapter_num: int):
    """‡∏î‡∏∂‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏à‡∏≤‡∏Å Vajirayana.org ‡∏ï‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏•‡∏Ç‡∏ï‡∏≠‡∏ô ‡πÅ‡∏•‡πâ‡∏ß‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° .txt"""
    thai_num = to_thai_num(chapter_num)
    url = BASE_URL.format(thai_num)
    print(f"üìñ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏î‡∏∂‡∏á: {url}")

    try:
        response = requests.get(url)
        response.encoding = "utf-8"

        if response.status_code != 200:
            print(f"‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏Ç‡πâ‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {chapter_num} (‡∏£‡∏´‡∏±‡∏™ {response.status_code})")
            return

        soup = BeautifulSoup(response.text, "html.parser")

        # ‡∏ä‡∏∑‡πà‡∏≠‡∏ö‡∏ó
        title_tag = soup.find("h1", class_="title")
        title = title_tag.get_text(strip=True) if title_tag else f"‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {chapter_num}"

        # ‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
        content_div = soup.find("div", class_="field-item")
        if not content_div:
            content_div = soup.find("div", class_="node-content")

        if not content_div:
            print(f"‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏Ç‡∏≠‡∏á‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {chapter_num}")
            return

        paragraphs = content_div.find_all("p")
        if paragraphs:
            text = "\n".join([p.get_text(strip=True) for p in paragraphs])
        else:
            text = content_div.get_text(separator="\n", strip=True)

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå
        os.makedirs(OUTPUT_DIR, exist_ok=True)
        file_path = os.path.join(OUTPUT_DIR, f"chapter{chapter_num}.txt")
        with open(file_path, "w", encoding="utf-8") as f:
            f.write(title + "\n\n" + text)

        print(f"‚úÖ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {chapter_num} ‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à ‚Üí {file_path}")

    except Exception as e:
        print(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {chapter_num}: {e}")

    time.sleep(1)


if __name__ == "__main__":
    start_chapter = 1
    end_chapter = 5  # üî∏ ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡πà‡∏≠‡∏ô 5 ‡∏ï‡∏≠‡∏ô
    print(f"üöÄ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏≤‡∏°‡∏Å‡πä‡∏Å ‡∏ï‡∏≠‡∏ô‡∏ó‡∏µ‡πà {start_chapter} ‡∏ñ‡∏∂‡∏á {end_chapter} ‡∏à‡∏≤‡∏Å Vajirayana.org\n")
    for i in range(start_chapter, end_chapter + 1):
        scrape_chapter(i)
    print("\nüéâ ‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î!")
